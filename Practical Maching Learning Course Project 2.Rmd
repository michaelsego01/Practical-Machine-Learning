---
title: "Practical Machine Learning Course Project 2"
author: "Michael"
date: "2024-05-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Processing
## Installing and loading R packages
```{r}
library(knitr)
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
library(kernlab)
library(rattle)
library(dplyr)
```

## Downloading Data
```{r}
traincsv <- read.csv("C:/Users/micha/OneDrive/Desktop/pml-training.csv")

testcsv <- read.csv("C:/Users/micha/OneDrive/Desktop/pml-testing.csv")
```

## Cleaning Data
### Removing columns that contain NA missing values in the training and testing datasets
```{r}
traincsv <- traincsv[, colSums(is.na(traincsv)) == 0] 
testcsv <- testcsv[, colSums(is.na(testcsv)) == 0]
```

### Removing unnecessary variables from the training dataset
```{r}
traincsv <- traincsv[,-c(1:7)]
```

### Removing near zero variance variables from the train dataset
```{r}
NZV <- nearZeroVar(traincsv)
traincsv <- traincsv[,-NZV]
```

# Cross validation
## Creating indeces that will be included in the training dataset
```{r}
set.seed(1234) #set seed for reporudicible purposes
train_ind <- createDataPartition(y=traincsv$classe, p=0.7, list=FALSE)
```

## Splitting up training data into a training and testing sets
```{r}
training <- traincsv[train_ind,]
valid <- traincsv[-train_ind,]
```

# Building and Testing Models

## Setting up control for the training data to use a 3-fold cross validation
```{r}
TrainControl <- trainControl(method="cv", number=3, verboseIter=F)
```

## Decision Tree
```{r}
dtmodel <- train(classe~., data=training, method="rpart", trControl = TrainControl, tuneLength = 4)
fancyRpartPlot(dtmodel$finalModel, main= "Model 1",type=2) # displaying the final decision tree model
```

### Predicting Decision Tree Model data
```{r}
dtprediction <- predict(dtmodel, valid)
dtmatrix <- confusionMatrix(dtprediction, factor(valid$classe))
dtmatrix
```

## Random Forest 
```{r}
rfmodel <- train(classe~., data=training, method="rf", trControl = TrainControl, tuneLength = 5)
plot(rfmodel) #display the final Random Forest model
```

### Predicting Random Forest Model data
```{r}
rfprediction <- predict(rfmodel, valid)
rfmatrix <- confusionMatrix(rfprediction, factor(valid$classe))
rfmatrix
```

# Results
## Comparing Accuracy & Out of Sample Error
```{r}
models <- c("Decision Tree", "Random Forest")
accuracy <- round(c(dtmatrix$overall[1], rfmatrix$overall[1]), 3)
OutErrSample <- 1 - accuracy
data.frame(accuracy = accuracy, OutErrSample = OutErrSample, row.names = models)
```
#### According to the table, the best model is the Random Forest model. 

## Predicting with Test dataset with the Random Forest Model for the 20 cases
```{r}
testpred <- predict(rfmodel, testcsv)
print(testpred)
```

